How a Toby program gets executed. (This is not complete yet.)

Toby is really two different things. First, it's programming language.

Second, it's a programming environment that features a TurtleGraphics
interface. In this environment, you can run Toby programs, or LOGO programs,
or theoretically any other program that would make sense.

A Toby program, or rather, a program written in the Toby programming language,
when run, is broken down into an abstract logic tree, which is then
interpreted. Anything, whether it looks like Toby source code, or LOGO, or
martian love notes, can be executed in this environment, so long as there is
a means of converting it into this abstract logic tree.

Rewrite #3 paved the way for alternate programming language support, with the
last.toby.parsers.Parser abstraction. The main environment would construct a
language-specific parser, and after that, it would communicate through the
abstracted Parser interface. In practice, this has some flaws.

First, and most obviously, the construction of a TobyParser was hardcoded, so
there wasn't a way to use an alternate language without a little bit of
redesign in the code that started the user's program a-runnin'.

Second, no one ever wrote a second parser. It was Toby or nothing, which is
probably why flaw #1 remained.

Third, even if you wrote a parser, the parser had to determine if the syntax
of the code to run was correct at the same time that it was building the
logic tree, which made for some seriously ugly code.

Ideally, a better system would separate the "compiling" of the program into
four steps: tokenizing, lexing, and parsing, and linking. The tokenizer should
read in characters from the source code and break them into basic units
(words, numbers, whitespace, newlines, comments, etc) called "tokens". The
lexer will take these tokens and evaluate them against a set of predefined
rules ("a 'conditional statement' is defined as something that starts with the
word 'if' followed by an 'expression'. An 'expression' is defined as..."). The
parser is told, by the lexer, that a given chunk of tokens are valid in the
context of some rule or another, and builds the logic tree based on the info
the lexer passes on. Catastrophic errors are mostly handled in the tokenizer
(IOExceptions, out of memory, etc), while syntax errors are handled in the
lexer. The Toby parser, if the lexer is working from a correct set of rules,
should almost never have an error (however, a parser for a one-pass language
like C might have to complain about undefined variables, etc). Finally, the
linker resolves symbols in the program (variables, functions, etc) by examining
the complete logic tree and makes sure that the logic tree is sane overall.

Here's how Toby rewrite #4 accomplishes this.

The Tokenizer is much like the one in rewrite #3, except it was written
completely from scratch. Since we didn't have to work around the limitations
of Java's built-in Tokenizer anymore, the "SaneTokenizer" class has gone away,
and we have a Tokenizer base class that is at once more flexible and more
convenient to use than we ever had previously. After creating an instance and
twiddling its basic configuration, the Tokenizer spits out basic chunks of
program: words, numbers, comments (single and multiline), whitespace, and
newlines. Beyond what you configure it to know (such as what defines a comment
or whitespace), it doesn't have any insight into any programming language. In
fact, we use the Tokenizer to parse Toby source code, XML (see xml.txt), and
the langfiles (see i18n.txt) at different times.

The Lexer is abstracted completely. There is no Toby, or LOGO, or Martian-
Love-Note specific code in it at all. What the Lexer does is read in an XML
file that defines the syntactic rules of a programming language (or the
"grammar" of the language. These XML files are called "grammar files"). The
metadata in the XML file specifies what valid syntax is, and where it is and
is not acceptable. The Lexer will instantiate a Tokenizer and configure it
according to the specification in the XML file.

The Parser is a state machine. Each element of a program that is specified in
the XML file has a handler associated with it. The handler is just the name of
a C-callable function in the parser. The Lexer will do a dynamic lookup on the
binary to find that symbol, and call it as a function. This function is passed
the related information, and it generates nodes for the logic tree, updates
relevant internal state, and returns.

The Linker is also abstract. While the Lexer doesn't have a specific knowledge
of a language beyond the metadata it is fed, the linker only sees the complete
logic tree, which doesn't have any language-specific info. The logic tree
will have symbols, etc that need to be resolved, however, and the Linker does
this resolving, and makes a final attempt to verify the integrity of the
logic tree's structure.

The XML file:

...must start with:

  <?xml version="1.0" encoding="iso-8859-1"?>

The version and encoding can be anything. The values are ignored, but need to
be there for future expansion. The XML parser is pretty hacky right now, but
we might improve it later if we're so inclined.  :)  Therefore, version should
be "1.0" and encoding should be...uh, whatever encoding you used.

After that, you need to have a language tag as your root node:

  <language name="foo" firstelement="BARTEXT">

...inside of that, you need to define language elements.

  <element name="BARTEXT">
     <repeat min="2" max="4">
        <reqword text="bar"/>
     </repeat>
  </element>

..."reqword" translates to English as "Require a word token" which means, for
a program to be a valid Foo program, it would need to be a file with one line
(and no newlines) that looked like this:

bar bar

(or, alternately, "bar bar bar", or "bar bar bar bar".)

That's not too useful in itself. First, you need to configure the tokenizer to
be more tolerant. Inside of <language>, you need a <tokenizer> tag:

  <tokenizer casesensitive="false"
             whitespacechars=" \t"
             ignorewhitespace="true"
             ignorenewlines="true"
             singlelinecommentstart="//"
             ignoresinglelinecomments="true"
             multilinecommentstart=""
             multilinecommentend=""
             ignoremultilinecomments="true"
             quotechars='"'
             escaping="true"
             escapechar="\\"
             convertnumstowords="false"
  />


Most of this speaks for itself, and most of it is passed through to the
corresponding Tokenizer methods (quotechars goes to Tokenizer::setQuoteChars(),
etc; refer to src/io/Tokenizer.h). Now that we're ignoring newlines and
whitespace, your Foo program can be "bar bar" with all the pretty formatting
you like.

If you want more than one line of "bar bar", you can take two approaches.
First, you should set ignorenewlines="false" in the <tokenizer> tag. Then:

    <element name="BARTEXT">
        <repeat min="0" max="n">
            <repeat min="2" max="4">
                <reqword text="bar"/>
            </repeat>
            <reqnewline/>
        </repeat>
    </element>


...this says you want between 0 and infinite (that is, any number of) lines of
text that have between 2 and four repetitions of the word "bar" followed by
a newline indicator.

Alternately, you can set this up as:

    <element name="TOPLEVEL">
        <repeat min="0" max="n">
            <reqelement name="BARTEXT"/>
        </repeat>

        <element name="BARTEXT">
            <repeat min="2" max="4">
                <reqword text="bar"/>
            </repeat>
            <reqnewline/>
        </element>
    </element>

...and set <language firstelement="TOPLEVEL">, so that it starts at the
correct element. Besides the fact you might need this separation for your
parser to handle state correctly (we're getting to that), it's also more
pleasing to the average programmer to build things in modules like this.
You'll note that we nested elements. This hierarchy also tends to be more
pleasing, but it serves two purposes. First, if BARTEXT is only ever used by
TOPLEVEL and no other elements, then there's no reason not to demonstrate a
close association like this. There's no limit on how many levels of nesting
you use. Also, this is a matter of efficiency. At runtime, the Lexer, when
looking for a required element, will check first the immediate children of
the element that is doing the requesting (so TOPLEVEL will look to see if
BARTEXT is one of its children when it hits the <reqelement> tag), and then
check for immediate children of the <language> tag, which are considered to be
global. All other tags are not examined, including siblings and children of
children, which cuts out a good portion of the namespace.

So, with our last example, what happens if I try to pass through a program
like this...?

--------snip--------
bar bar bar
bar bar bar bar
bar bar bar
bar bar bar bar bar
--------snip--------

The lexer will accept ("swallow") the first three words, then the newline,
then the four, newline, then the three, newline, then...uhoh. It gets four
words, expects a newline, and sees another word instead. In such a case, it
cannot swallow the last line at all because it did not totally match the rule.
Tokens are only swallowed when they are verified to absolutely match a rule.
The TOPLEVEL element's rule succeeds, however, even though the entire program
wasn't swallowed, since it got what it wanted: between 0 and n occurences of
BARTEXT. It got three valid occurences, in this case, which are swallowed, and
the next rule, if there was one, would start with the beginning of line 4
(the one with five "bar" words), since none of those tokens were swallowed.

So wait, there's junk at the end of the "program", but it still succeeds? Yes.
You can fix this behaviour as such:

    <element name="TOPLEVEL">
        <repeat min="0" max="n">
            <reqelement name="BARTEXT"/>
        </repeat>
        <reqeof/>

        <element name="BARTEXT">
            <repeat min="2" max="4">
                <reqword text="bar"/>
            </repeat>
            <reqnewline/>
        </element>
    </element>


...all we added was <reqeof/>, which means that we need to get an arbitrary
number of BARTEXT elements, followed by the end of the program's file. With
that, our program would not swallow the last line in BARTEXT, which means that
the <repeat> will succeed, but then the <reqeof/> would fail, which means that
nothing actually gets swallowed, but more importantly, the lexer acknoledges
that the program is bogus, and generation of the logic tree stops.

Wait...generation of the logic tree? Who's generating the logic tree? In this
case, nobody is. You need a parser to generate a tree; the lexer just verifies
the syntax and directs the parser as to what's going on. The Parser is not
written in XML; it is usually written mostly in C++. Actually, it can be
written in any language that can be called from C (since we need to call it by
looking up function names dynamically). Currently, the parser needs to be
statically linked into the main binary, but it's relatively trivial to make
parsers into shared objects at some later point.

The Parser is a state machine made up of a collection of "handlers", which are
just C-callable functions. The lexer, when it finds that an element is
valid, will swallow the tokens and call the most convenient handler in the
Parser. The Parser will then update its state based on this call, and
perhaps build more onto the logic tree. It can alert the lexer of errors and
warnings.

The function signature that your Parser uses looks like this:

!!! write me.



The reference manual for each tag you can use is in grammar_tags.txt, and the
grammars/ directory has the current set of programming language grammars,
which are excellent practical examples.

If you build the test suite ("make tests"), then you can pass a grammar XML
file to bin/test/grammar and it will check the file for basic sanity. If a
grammar file gives any warnings in this test, its behaviour at runtime is
undefined. You have been warned.


The future:

* Theoretically, we could either add a "tree walker" or retrofit the
  interpreter to output code. This could lead to outputting an XML equivalent
  of the logic tree, or native code generation, or even (heaven forbid!) a
  GCC frontend. It'd be funny to pass a Toby program in and have it spit out
  a LOGO equivalent. Better yet, we could generate Java byte code that creates
  an Applet of the given program for execution on a webpage (maybe rewrite #3
  is salvagable in this respect).
* The grammar files should have an associated DTD, so that invalid XML just
  refuses to fly in the first place, and external tools can be used to test
  validity; that seems sa[nf]er to me.
* There are systems (PalmOS comes to mind) that could do with more efficiency
  and less abstraction. There are also systems (PalmOS comes to mind, again)
  that don't have dynamic loading. These systems will need an alternative to
  the XML file. For this, I'd recommend a standalone program that will parse
  the XML and generate C++ code that accomplishes the program lexing and
  parsing. The generated C++ can be statically linked into the main binary.
  This eliminates overhead and the dynamic lookups.
* For dynamic use, we should make it so the parser can be a shared library
  and therefore not compiled into the main binary.

// end of code_execution.txt ...

